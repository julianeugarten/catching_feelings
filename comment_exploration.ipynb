{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "23cef9a5-5e09-4de8-adb5-665666781ea6",
      "metadata": {
        "id": "23cef9a5-5e09-4de8-adb5-665666781ea6"
      },
      "source": [
        "## Exploring the Dataset of Comments\n",
        "This code was used to explore a dataset that contains 25.970 comments left on fanfiction included in the existing *MythFic* dataset. This comment-dataset is 984.369 words in total. To protect the privacy of the fanfiction community, the comment data used in the *Catching Feelings* project will not be made available for reuse, but you can use the scraping code available on our Github to collect your own dataset, then use this code to explore."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2956cc5f-83c4-4058-827e-a276536c8be6",
      "metadata": {
        "id": "2956cc5f-83c4-4058-827e-a276536c8be6"
      },
      "source": [
        "This notebook explores the following questions:\n",
        "- How many characters and words are in each commments? I also provide some descriptive statistics of these numbers.\n",
        "- Which languages are these comments predominantly written in?\n",
        "- What are the most frequent words in the comments?\n",
        "- What aspects do readers comment on? I explore this by looking at the most frequent words and identifying most frequent noun chunks with SpaCy.\n",
        "- I also explore a Top2Vec topic model of all comments over 300 characters in length. Because this model is very large I have not (yet) shared it online. Email the author(s) to receive a WeTransfer link.\n",
        "\n",
        "This notebook also contains the filtering and selection of the comments for annotation.\n",
        "\n",
        "**Disclaimer:** Chat-GPT helped with some of the code in this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31fa89c3-dbd2-4fe5-a5d0-12f437bb9a6f",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "31fa89c3-dbd2-4fe5-a5d0-12f437bb9a6f"
      },
      "source": [
        "### Preliminaries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install langid\n",
        "! pip install langdetect\n",
        "! pip install top2vec"
      ],
      "metadata": {
        "id": "4fJtN0LKbodF"
      },
      "id": "4fJtN0LKbodF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "dbaa211a-5984-4424-b27b-76881d3c2f9b",
      "metadata": {
        "id": "dbaa211a-5984-4424-b27b-76881d3c2f9b"
      },
      "outputs": [],
      "source": [
        "# requirements\n",
        "import csv\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from langdetect import detect\n",
        "from collections import Counter\n",
        "import langid\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import clear_output, display\n",
        "from top2vec import Top2Vec\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "323becc5",
      "metadata": {
        "id": "323becc5"
      },
      "outputs": [],
      "source": [
        "from langid.langid import LanguageIdentifier, model\n",
        "language_identifier = LanguageIdentifier.from_modelstring(model, norm_probs=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "J4SUhZrp1RtI"
      },
      "id": "J4SUhZrp1RtI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a1ef8b92-abf4-41d8-b811-e2df5577e2a0",
      "metadata": {
        "id": "a1ef8b92-abf4-41d8-b811-e2df5577e2a0"
      },
      "outputs": [],
      "source": [
        "# loading your data\n",
        "df = pd.read_csv('filename.csv', sep=';')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbc6245e-2356-4c11-a480-7a8e1e8f2710",
      "metadata": {
        "id": "cbc6245e-2356-4c11-a480-7a8e1e8f2710"
      },
      "outputs": [],
      "source": [
        "# use this to check whether the data has been loaded\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab76f5c5-41fe-4ccb-8058-dc5d4df6cd40",
      "metadata": {
        "id": "ab76f5c5-41fe-4ccb-8058-dc5d4df6cd40"
      },
      "outputs": [],
      "source": [
        "# how many comments are in the dataframe?\n",
        "len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ce03b7d1",
      "metadata": {
        "id": "ce03b7d1"
      },
      "outputs": [],
      "source": [
        "df['Comment']= df['Comment'].astype(str)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7a75e87-7518-4a3f-b663-e26ff936f399",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "f7a75e87-7518-4a3f-b663-e26ff936f399"
      },
      "source": [
        "### Characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "f7faba69-fb69-4826-9ede-d145e8303c93",
      "metadata": {
        "id": "f7faba69-fb69-4826-9ede-d145e8303c93"
      },
      "outputs": [],
      "source": [
        "# calculate how many characters are in each comment and add a characters-column\n",
        "df['chars'] = df['Comment'].str.len()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44a379d9-667a-42d7-9152-300ef0f2f088",
      "metadata": {
        "id": "44a379d9-667a-42d7-9152-300ef0f2f088"
      },
      "outputs": [],
      "source": [
        "# calculate some descriptive stats on the number of characters used in each comment\n",
        "df['chars'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbe4e637-577c-4b66-95da-3f67926a2325",
      "metadata": {
        "id": "fbe4e637-577c-4b66-95da-3f67926a2325"
      },
      "outputs": [],
      "source": [
        "# how many characters in total?\n",
        "df['chars'].sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acb9c792-3343-4714-a299-38a08de30146",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "acb9c792-3343-4714-a299-38a08de30146"
      },
      "source": [
        "### Wordcount"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "e50e4ed1-25c5-4fa1-b47a-fcf3598e8409",
      "metadata": {
        "id": "e50e4ed1-25c5-4fa1-b47a-fcf3598e8409"
      },
      "outputs": [],
      "source": [
        "# calculate how many words are in each comment and add a wordcount-column\n",
        "df['wordcount'] = df['Comment'].str.split().str.len()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dace58fe-81d1-4bb2-abbc-d35878a3640c",
      "metadata": {
        "id": "dace58fe-81d1-4bb2-abbc-d35878a3640c"
      },
      "outputs": [],
      "source": [
        "# calculate some descriptive stats on the wordcount of each comment\n",
        "df['wordcount'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a091e62-b280-41f1-ab4e-990a5c98bacd",
      "metadata": {
        "id": "2a091e62-b280-41f1-ab4e-990a5c98bacd"
      },
      "outputs": [],
      "source": [
        "# calculate total wordcount\n",
        "df['wordcount'].sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a368c393-66b9-4cce-95c3-16e08b29cc05",
      "metadata": {
        "id": "a368c393-66b9-4cce-95c3-16e08b29cc05"
      },
      "source": [
        "### Most Frequent Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5392ae8b-0e28-4140-854b-ffc83142f686",
      "metadata": {
        "id": "5392ae8b-0e28-4140-854b-ffc83142f686"
      },
      "outputs": [],
      "source": [
        "strings = df[\"Comment\"].astype(str)\n",
        "Counter(\" \".join(strings).split()).most_common(100)\n",
        "# this is not very informative."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "839dc0ba",
      "metadata": {
        "id": "839dc0ba"
      },
      "source": [
        "### Language Identification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14170090",
      "metadata": {
        "id": "14170090"
      },
      "outputs": [],
      "source": [
        "def identify_language(text):\n",
        "    return language_identifier.classify(text)\n",
        "\n",
        "def add_language_columns(df, text_column):\n",
        "    languages = []\n",
        "    confidences = []\n",
        "    for text in df['Comment']:\n",
        "        language, confidence = identify_language(text)\n",
        "        languages.append(language)\n",
        "        confidences.append(confidence)\n",
        "    df['Language'] = languages\n",
        "    df['Confidence'] = confidences\n",
        "    return df\n",
        "\n",
        "# Add language columns to the DataFrame\n",
        "df = add_language_columns(df, 'Text')\n",
        "\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "831e8566",
      "metadata": {
        "id": "831e8566"
      },
      "outputs": [],
      "source": [
        "# Here's some code to examine specific Language-labels\n",
        "# == means check for exact label, != means check for absence of label\n",
        "\n",
        "len(\n",
        "    df\n",
        "    .loc[lambda df: (df['Language'] != 'en')]\n",
        "    .loc[lambda df: (df['chars'] < 300)]\n",
        ")\n",
        "\n",
        "# Note: 24.444 comments in the *Catching Feelings* dataset were identified as English\n",
        "# 1526 as non-English\n",
        "# For comments over 300 characters, it's 19.941 in English and 1476 in non-English."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23197707",
      "metadata": {
        "id": "23197707"
      },
      "outputs": [],
      "source": [
        "df['Language'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f994e557",
      "metadata": {
        "id": "f994e557"
      },
      "outputs": [],
      "source": [
        "df.sort_values(by=['Language'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cc9d792",
      "metadata": {
        "id": "8cc9d792"
      },
      "source": [
        "### Identifying the Most Common Noun Chunks with SpaCy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01091bb1",
      "metadata": {
        "id": "01091bb1"
      },
      "outputs": [],
      "source": [
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function to extract noun chunks from text\n",
        "def extract_noun_chunks(text):\n",
        "    doc = nlp(text)\n",
        "    return [chunk.text for chunk in doc.noun_chunks]\n",
        "\n",
        "# Apply noun chunk extraction to each row in the 'Text' column\n",
        "df['Noun_chunks'] = df['Comment'].apply(extract_noun_chunks)\n",
        "\n",
        "# Print the DataFrame with the extracted noun chunks\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51809822",
      "metadata": {
        "id": "51809822"
      },
      "outputs": [],
      "source": [
        "# Concatenate all noun chunks from all rows into a single list\n",
        "all_noun_chunks = [chunk for row in df['Noun_chunks'] for chunk in row]\n",
        "\n",
        "# Count the occurrences of each unique noun chunk\n",
        "noun_chunk_counts = Counter(all_noun_chunks)\n",
        "\n",
        "# Find the most frequent noun chunks\n",
        "most_common_noun_chunks = noun_chunk_counts.most_common()\n",
        "\n",
        "print(most_common_noun_chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fbd1b93",
      "metadata": {
        "id": "7fbd1b93"
      },
      "source": [
        "Some of the most frequent noun chunks that may point to aspects in the *Catching Feelings* data include:\n",
        "- Achilles, Persephone, Hades, Patroclus (aspect: character)\n",
        "- the story, the end, the ending, a happy ending, the plot (aspect: events)\n",
        "- the world, Troy, the underworld (aspect: storyworld)\n",
        "- your writing, writing, imagery (aspect: style)\n",
        "- love, <3, my heart, tears, awe, fun, omg, pain,  (aspect: reader response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d13640e",
      "metadata": {
        "id": "9d13640e"
      },
      "source": [
        "#### You may want to save this dataframe with all the acquired metadata as a csv.*italicized text*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "4504062f",
      "metadata": {
        "id": "4504062f"
      },
      "outputs": [],
      "source": [
        "df.to_csv('comments+metadata.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8a7e0ce",
      "metadata": {
        "id": "b8a7e0ce"
      },
      "source": [
        "## Exploring a Top2Vec Topic Model of the Lengthier Comments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94cb8968",
      "metadata": {
        "id": "94cb8968"
      },
      "outputs": [],
      "source": [
        "# this is a topic model of all comments over 300 characters long\n",
        "# it was created with Top2Vec\n",
        "\n",
        "model = Top2Vec.load(\"filtered_comments\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebff0410",
      "metadata": {
        "id": "ebff0410"
      },
      "outputs": [],
      "source": [
        "model.get_num_topics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b9434b8",
      "metadata": {
        "id": "8b9434b8"
      },
      "outputs": [],
      "source": [
        "model.get_topics(25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e1e1d7b",
      "metadata": {
        "id": "1e1e1d7b"
      },
      "outputs": [],
      "source": [
        "# You can visualize any topic like this\n",
        "# I don't really find these easy to interpret\n",
        "model.generate_topic_wordcloud(6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdc0afb9",
      "metadata": {
        "id": "fdc0afb9"
      },
      "outputs": [],
      "source": [
        "# and take a closer look at the words in any topic\n",
        "topic_words, word_scores, topics = model.get_topics(2)\n",
        "for words, scores, num in zip(topic_words[1:], word_scores[1:], topics[1:]):\n",
        "    print(f\"Topic {num}\")\n",
        "    for word, score in zip(words, scores):\n",
        "        print(word, score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ac051ad",
      "metadata": {
        "id": "1ac051ad"
      },
      "outputs": [],
      "source": [
        "# you can also see how many documents are in each topic\n",
        "topic_sizes, topic_nums = model.get_topic_sizes()\n",
        "for topic_size, topic_num in zip(topic_sizes[:45], topic_nums[:45]):\n",
        "    print(f\"Topic Num {topic_num} has {topic_size} documents.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c3b587d",
      "metadata": {
        "id": "1c3b587d"
      },
      "source": [
        "## Creating the Annotation Set for *Catching Feelings*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4d2e7bb",
      "metadata": {
        "id": "a4d2e7bb"
      },
      "source": [
        "- filtered by length between 100 and 4000 characters\n",
        " - this leads to a subset of 13073 comments, or about half of the total\n",
        "- with a relatively high confidence that the language is English\n",
        "- randomly select 1.000 comments"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1612c8c8",
      "metadata": {
        "id": "1612c8c8"
      },
      "source": [
        "### Length selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "6c137b31",
      "metadata": {
        "id": "6c137b31"
      },
      "outputs": [],
      "source": [
        "# filter by length\n",
        "length_filter = df[(df['chars'] >= 100) & (df['chars'] <= 4000)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed0f66ce",
      "metadata": {
        "id": "ed0f66ce"
      },
      "outputs": [],
      "source": [
        "# check how many comments are left after the length filter\n",
        "len(length_filter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "4f22d0b3",
      "metadata": {
        "id": "4f22d0b3"
      },
      "outputs": [],
      "source": [
        "language_filter = length_filter[(length_filter['Language'] == 'en')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddc66d5a",
      "metadata": {
        "id": "ddc66d5a"
      },
      "outputs": [],
      "source": [
        "len(language_filter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "914d7d37",
      "metadata": {
        "id": "914d7d37"
      },
      "outputs": [],
      "source": [
        "language_filter.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "45a4ac1a",
      "metadata": {
        "id": "45a4ac1a"
      },
      "outputs": [],
      "source": [
        "probability_filter = language_filter[(language_filter['Confidence'] >= 0.9)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bcfe265",
      "metadata": {
        "id": "6bcfe265"
      },
      "outputs": [],
      "source": [
        "len(probability_filter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "102df4d9",
      "metadata": {
        "id": "102df4d9"
      },
      "outputs": [],
      "source": [
        "filtered_set = probability_filter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7d1ccde",
      "metadata": {
        "id": "f7d1ccde"
      },
      "outputs": [],
      "source": [
        "filtered_set.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f8281f5",
      "metadata": {
        "id": "2f8281f5"
      },
      "source": [
        "## Now let's sample 1000 and save them as txt-files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85e89cbc",
      "metadata": {
        "id": "85e89cbc"
      },
      "outputs": [],
      "source": [
        "random_sample_filtered_set = filtered_set.sample(n=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94419eb5",
      "metadata": {
        "id": "94419eb5"
      },
      "outputs": [],
      "source": [
        "random_sample_filtered_set.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38d2febe",
      "metadata": {
        "id": "38d2febe"
      },
      "outputs": [],
      "source": [
        "# Create a folder named 'annotation_set' if it doesn't exist\n",
        "folder_path = 'annotation_set'\n",
        "if not os.path.exists(folder_path):\n",
        "    os.makedirs(folder_path)\n",
        "\n",
        "# Iterate over the 'Comments' column and write each comment to a separate text file\n",
        "for index, comment in enumerate(random_sample_filtered_set['Comment']):\n",
        "    file_name = f'comment_{index}.txt'\n",
        "    file_path = os.path.join(folder_path, file_name)\n",
        "    with open(file_path, 'w', encoding='utf-8') as file:\n",
        "        file.write(comment)\n",
        "\n",
        "print(\"Text files have been created in the 'annotation_set' folder.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}